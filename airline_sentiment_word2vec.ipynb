{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Contraction map\n",
    "contraction_map = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"can't\": \"cannot\", \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\", \"it's\": \"it is\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "    \"won't\": \"will not\", \"wouldn't\": \"would not\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\",\n",
    "    \"didn't\": \"did not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\", \"we've\": \"we have\", \"they've\": \"they have\", \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\", \"we'll\": \"we will\", \"they'll\": \"they will\", \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\", \"he'd\": \"he would\", \"she'd\": \"she would\", \"they'd\": \"they would\",\n",
    "    \"we'd\": \"we would\", \"let's\": \"let us\", \"who's\": \"who is\", \"what's\": \"what is\",\n",
    "    \"could've\": \"could have\", \"would've\": \"would have\", \"should've\": \"should have\"\n",
    "}\n",
    "\n",
    "# Basic stopwords\n",
    "stop_words = {\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\",\n",
    "    \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\",\n",
    "    \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\",\n",
    "    \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\",\n",
    "    \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "    \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\",\n",
    "    \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\",\n",
    "    \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "    \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\",\n",
    "    \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def expand_contractions(text):\n",
    "    pattern = re.compile('({})'.format('|'.join(re.escape(k) for k in contraction_map.keys())), flags=re.IGNORECASE)\n",
    "    return pattern.sub(lambda x: contraction_map[x.group().lower()], text)\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    words = text.split()\n",
    "    return [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"Airline-Sentiment-2-w-AA.csv\")\n",
    "df['cleaned'] = df['text'].apply(preprocess_tweet)\n",
    "df[['text', 'cleaned']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Word2Vec model\n",
    "w2v = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4856c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert tweets to vectors\n",
    "def tweet_to_vec(words, model, vector_size=300):\n",
    "    valid_words = [model[word] for word in words if word in model]\n",
    "    if not valid_words:\n",
    "        return [0.0] * vector_size\n",
    "    return list(sum(valid_words) / len(valid_words))\n",
    "\n",
    "X = df['cleaned'].apply(lambda x: tweet_to_vec(x, w2v)).tolist()\n",
    "y = df['airline_sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d740ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train/test split and train classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09163c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction function\n",
    "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
    "    processed = preprocess_tweet(tweet)\n",
    "    vec = tweet_to_vec(processed, w2v_model)\n",
    "    return model.predict([vec])[0]\n",
    "\n",
    "# Example\n",
    "predict_tweet_sentiment(clf, w2v, \"I love this airline, great service!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
